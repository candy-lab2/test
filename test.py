import streamlit as st
import base64
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# =========================
# Qwen ãƒ¢ãƒ‡ãƒ«è¨­å®š
# =========================
MODEL_NAME = "Qwen/Qwen3-8B"
# è»½ãã—ãŸã„ãªã‚‰:
# MODEL_NAME = "Qwen/Qwen3-4B"


@st.cache_resource
def load_model_and_tokenizer():
    """
    Qwen ãƒ¢ãƒ‡ãƒ«ã¨ Tokenizer ã‚’ 4bit é‡å­åŒ–ã§ãƒ­ãƒ¼ãƒ‰ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰
    """
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_compute_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
    )

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        device_map="auto",
        quantization_config=bnb_config,
    )
    return tokenizer, model


def summarize_qwen3(text: str, target_chars: int = 120) -> str:
    """
    Qwen3 ã‚’ä½¿ã£ã¦æ—¥æœ¬èªè¦ç´„ã™ã‚‹ï¼ˆè‹±èªç¦æ­¢ã€<think> ã‚‚å‰Šã‚‹ï¼‰ã€‚
    """
    tokenizer, model = load_model_and_tokenizer()

    user_prompt = (
        f"æ¬¡ã®æ–‡ç« ã‚’ã€æ—¥æœ¬èªã§ã€ã ã„ãŸã„ {target_chars} æ–‡å­—ç¨‹åº¦ã«è‡ªç„¶ã«è¦ç´„ã—ã¦ãã ã•ã„ã€‚\n"
        f"ãƒ»å‡ºåŠ›ã¯å¿…ãšæ—¥æœ¬èªã®ã¿\n"
        f"ãƒ»è‹±èªã‚„æ¨è«–éç¨‹ï¼ˆ<think>ãªã©ï¼‰ã‚’å‡ºåŠ›ã—ãªã„\n"
        f"ãƒ»çµæœã ã‘ç°¡æ½”ã«\n\n"
        f"ã€æ–‡ç« ã€‘\n{text}\n\nã€è¦ç´„ã€‘"
    )

    messages = [
        {
            "role": "system",
            "content": (
                "ã‚ãªãŸã¯æ—¥æœ¬èªå°‚ç”¨ã®è¦ç´„ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚"
                "è‹±èªã‚„ä»–ã®è¨€èªã€æ¨è«–éç¨‹ãƒ»è‡ªå·±ã‚³ãƒ¡ãƒ³ãƒˆã‚’çµ¶å¯¾ã«å‡ºåŠ›ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚"
                "å›ç­”ã¯ã€è¦ç´„ã€‘ã®å¾Œã®æ–‡ç« ã®ã¿ã¨ã—ã¾ã™ã€‚"
            ),
        },
        {"role": "user", "content": user_prompt},
    ]

    chat_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
    )

    inputs = tokenizer(chat_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        generated = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.3,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
        )

    gen_ids = generated[0][inputs["input_ids"].shape[1]:]
    output = tokenizer.decode(gen_ids, skip_special_tokens=True)

    # ã€Œã€è¦ç´„ã€‘ã€ä»¥é™ã ã‘æŠœãå‡ºã™
    if "ã€è¦ç´„ã€‘" in output:
        output = output.split("ã€è¦ç´„ã€‘", 1)[-1].strip()

    # <think> ã‚„è‹±å­—ã‚’å¿µã®ãŸã‚é™¤å»
    output = re.sub(r"<think>.*?</think>", "", output, flags=re.DOTALL).strip()
    output = re.sub(r"[a-zA-Z]+", "", output).strip()

    return output


# =========================
# ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒªãƒ³ã‚¯ä½œæˆ
# =========================
def create_download_link(content, filename):
    b64 = base64.b64encode(content.encode()).decode()
    href = f'<a href="data:file/txt;base64,{b64}" download="{filename}">è¦ç´„çµæœã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰</a>'
    return href


# =========================
# Streamlit UI æœ¬ä½“
# =========================
def main():
    st.title("è¦ç´„")

    # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
    if "summary" not in st.session_state:
        st.session_state["summary"] = ""

    option = st.radio("é¸æŠã—ã¦ãã ã•ã„", ("ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›", "ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"))

    # ------- å…¥åŠ›éƒ¨åˆ† -------
    text = ""

    if option == "ãƒ†ã‚­ã‚¹ãƒˆå…¥åŠ›":
        # ğŸ”¹ã“ã“ãŒã€Œå…¥åŠ›ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆãƒœãƒƒã‚¯ã‚¹ã€
        text = st.text_area(
            "ãƒ†ã‚­ã‚¹ãƒˆã‚’å…¥åŠ›ã—ã¦ãã ã•ã„",
            key="input_text",          # key ã‚’æ˜ç¤ºã—ã¦çŠ¶æ…‹è¡çªã‚’é¿ã‘ã‚‹
            height=200,
        )
    else:
        uploaded_file = st.file_uploader("ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆ.txtï¼‰", type=["txt"])
        if uploaded_file is not None:
            file_bytes = uploaded_file.read()
            try:
                text = file_bytes.decode("utf-8")
            except UnicodeDecodeError:
                try:
                    text = file_bytes.decode("shift_jis")
                except UnicodeDecodeError:
                    st.error("UTF-8 / Shift_JIS ã¨ã—ã¦èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸã€‚")
                    text = ""
            # èª­ã¿è¾¼ã‚“ã å†…å®¹ã‚’ã€Œç·¨é›†ã§ãã‚‹ã€å½¢ã§è¡¨ç¤º
            text = st.text_area(
                "èª­ã¿è¾¼ã‚“ã ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå¿…è¦ãªã‚‰ç·¨é›†ã—ã¦ãã ã•ã„ï¼‰",
                value=text,
                key="uploaded_text",
                height=200,
            )

    # --- æ–‡å­—æ•°æŒ‡å®š ---
    target_chars = st.selectbox("æ–‡å­—æ•°æŒ‡å®šï¼ˆç›®å®‰ï¼‰", [100, 200, 300], index=0)

    # --- è¦ç´„ãƒœã‚¿ãƒ³ ---
    if st.button("è¦ç´„ã™ã‚‹"):
        if not text.strip():
            st.warning("ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™ã€‚å…¥åŠ›ã™ã‚‹ã‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
        else:
            with st.spinner("Qwen3 ã§è¦ç´„ä¸­..."):
                summary = summarize_qwen3(text, target_chars=target_chars)
                st.session_state["summary"] = summary

    # ------- è¦ç´„çµæœè¡¨ç¤º -------
    st.subheader("è¦ç´„çµæœ")
    summary = st.session_state.get("summary", "")

    # ã“ã£ã¡ã¯ã€Œå‡ºåŠ›ç”¨ã€ãªã®ã§ç·¨é›†ã§ãã¦ã‚‚ã„ã„ã—ã€
    # ç·¨é›†ã•ã›ãŸããªã„ãªã‚‰ disabled=True ã‚’ä»˜ã‘ã‚‹
    st.text_area(
        "è¦ç´„çµæœ",
        value=summary,
        height=200,
        key="summary_box",
    )
    st.text(f"æ–‡å­—æ•°ï¼š{len(summary)}")

    if summary:
        st.markdown(
            create_download_link(summary, "summary.txt"),
            unsafe_allow_html=True,
        )


if __name__ == "__main__":
    main()
